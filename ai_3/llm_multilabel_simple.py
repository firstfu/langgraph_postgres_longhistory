# =============================================================================
# ç°¡åŒ–ç‰ˆ LLM é¢¨æ ¼å¤šæ¨™ç±¤æ–‡æœ¬åˆ†é¡æ¨¡å‹
# åŠŸèƒ½ï¼šæ¨¡æ“¬ Transformer æ¶æ§‹çš„å¤šæ¨™ç±¤åˆ†é¡
# æŠ€è¡“ï¼šè‡ªå®šç¾© Transformer + ä¸­æ–‡æ–‡æœ¬è™•ç† + å¤šæ¨™ç±¤åˆ†é¡
# å„ªå‹¢ï¼šä¸éœ€è¦ä¸‹è¼‰å¤§å‹é è¨“ç·´æ¨¡å‹ï¼Œå¿«é€Ÿæ¸¬è©¦å’Œå­¸ç¿’
# =============================================================================

import matplotlib

matplotlib.use('Agg')  # ä½¿ç”¨éäº’å‹•å¼å¾Œç«¯é¿å…é¡¯ç¤ºå•é¡Œ
import warnings

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (classification_report, f1_score, hamming_loss,
                             jaccard_score)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from torch.utils.data import DataLoader, Dataset

warnings.filterwarnings('ignore')

# å˜—è©¦å°å…¥ jiebaï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ç°¡å–®çš„æ–‡æœ¬è™•ç†
try:
    import jieba
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False

# è¨­å®šéš¨æ©Ÿç¨®å­ç¢ºä¿çµæœå¯é‡ç¾
torch.manual_seed(42)
np.random.seed(42)

# 1. ä¸­æ–‡æ–‡æœ¬é è™•ç†
class ChineseTextProcessor:
    def __init__(self):
        # åœç”¨è©åˆ—è¡¨
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ ', 'æœƒ', 'ç€', 'æ²’æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'é€™', 'å¹´', 'é‚£', 'ç¾åœ¨', 'å¯ä»¥', 'ä½†æ˜¯', 'é€™å€‹', 'ä¸­', 'å¤§', 'ç‚º',
            'ä¾†', 'å€‹', 'èƒ½', 'å°', 'æ›´', 'ç­‰', 'é‚„', 'é€™æ¨£', 'é€²è¡Œ', 'ç›¸é—œ', 'ä»Šå¤©',
            'ä»Šæ—¥', 'å®£å¸ƒ', 'è¡¨ç¤º', 'æŒ‡å‡º', 'èªç‚º', 'ç™¼ç¾', 'é¡¯ç¤º', 'å ±å‘Š'
        }

    def process_text(self, text):
        """è™•ç†ä¸­æ–‡æ–‡æœ¬ï¼šä½¿ç”¨ jieba åˆ†è©æˆ–åŸºæœ¬å­—ç¬¦è™•ç†"""
        if JIEBA_AVAILABLE:
            # ä½¿ç”¨ jieba åˆ†è©
            words = jieba.cut(text)
            filtered_words = [
                word.strip() for word in words
                if word.strip() and len(word.strip()) > 1 and word.strip() not in self.stop_words
            ]
            return ' '.join(filtered_words)
        else:
            # ç°¡å–®çš„å­—ç¬¦åˆ†å‰²
            words = []
            i = 0
            while i < len(text):
                # å˜—è©¦æå–2-3å­—è©
                for length in [3, 2, 1]:
                    if i + length <= len(text):
                        word = text[i:i+length]
                        if word not in self.stop_words and len(word.strip()) > 0:
                            words.append(word)
                            i += length
                            break
                else:
                    i += 1

            return ' '.join(words)

# 2. æº–å‚™è¨“ç·´æ•¸æ“š
def create_comprehensive_data():
    """å‰µå»ºæ›´è±å¯Œçš„å¤šæ¨™ç±¤è¨“ç·´æ•¸æ“š"""
    texts = [
        # æ”¿æ²»ç›¸é—œ
        "ç¸½çµ±å®£å¸ƒæ–°çš„ç¶“æ¿Ÿåˆºæ¿€æ”¿ç­–ä»¥ä¿ƒé€²åœ‹å®¶ç™¼å±•",
        "åœ‹æœƒè­°å“¡è¨è«–ä¿®æ”¹ç¨…æ”¶æ³•æ¡ˆçš„ç›¸é—œæ¢æ¬¾",
        "å¤–äº¤éƒ¨é•·èˆ‡å„åœ‹ä»£è¡¨èˆ‰è¡Œé‡è¦æœƒè«‡",
        "æ”¿åºœæ¨å‡ºå…¨æ°‘å¥åº·ä¿éšªæ”¹é©æ–¹æ¡ˆ",
        "é¸èˆ‰å§”å“¡æœƒå…¬å¸ƒæœ€æ–°çš„æ°‘æ„èª¿æŸ¥çµæœ",
        "ç«‹æ³•é™¢é€šéæ–°çš„ç’°å¢ƒä¿è­·æ³•æ¡ˆ",
        "å¸‚é•·å®£å¸ƒéƒ½å¸‚æ›´æ–°è¨ˆç•«æŠ•è³‡é‡‘é¡",
        "ç¸½ç†å‡ºè¨ªé„°åœ‹è¨è«–è²¿æ˜“åˆä½œå”è­°",

        # ç¶“æ¿Ÿç›¸é—œ
        "è‚¡å¸‚ä»Šæ—¥æ”¶ç›¤å‰µä¸‹æ­·å²æ–°é«˜ç´€éŒ„",
        "å¤®è¡Œå®£å¸ƒèª¿é™åŸºæº–åˆ©ç‡åˆºæ¿€ç¶“æ¿Ÿ",
        "åœ‹éš›æ²¹åƒ¹ä¸Šæ¼²å½±éŸ¿é€šè†¨é æœŸ",
        "æˆ¿åœ°ç”¢å¸‚å ´äº¤æ˜“é‡å¤§å¹…å¢åŠ ",
        "ä¼æ¥­è²¡å ±é¡¯ç¤ºç²åˆ©å¤§å¹…æˆé•·",
        "å°±æ¥­å¸‚å ´å‡ºç¾æ˜é¡¯å¾©ç”¦è·¡è±¡",
        "æ¶ˆè²»è€…ç‰©åƒ¹æŒ‡æ•¸æŒçºŒä¸Šå‡",
        "åŒ¯ç‡æ³¢å‹•å°å‡ºå£ç”¢æ¥­é€ æˆè¡æ“Š",

        # ç§‘æŠ€ç›¸é—œ
        "äººå·¥æ™ºæ…§æŠ€è¡“åœ¨é†«ç™‚è¨ºæ–·é ˜åŸŸå–å¾—çªç ´",
        "5Gç¶²è·¯å»ºè¨­å¸¶å‹•ç›¸é—œç”¢æ¥­ç™¼å±•",
        "é‡å­è¨ˆç®—ç ”ç©¶ç²å¾—é‡å¤§é€²å±•",
        "å€å¡ŠéˆæŠ€è¡“æ‡‰ç”¨æ–¼é‡‘èæœå‹™å‰µæ–°",
        "è‡ªå‹•é§•é§›æ±½è»Šå®Œæˆè·¯æ¸¬é©—è­‰",
        "è™›æ“¬å¯¦å¢ƒè¨­å‚™éŠ·é‡å‰µæ–°é«˜",
        "æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å„ªåŒ–ç”Ÿç”¢æ•ˆç‡",
        "é›²ç«¯é‹ç®—æœå‹™å¸‚å ´å¿«é€Ÿæ“´å¼µ",

        # é«”è‚²ç›¸é—œ
        "NBAç¸½å† è»è³½é€²å…¥æ±ºæˆ°æ™‚åˆ»",
        "ä¸–ç•Œç›ƒè¶³çƒè³½å°çµ„è³½æ¿€çƒˆç«¶çˆ­",
        "å¥§é‹æ¸¸æ³³æ¯”è³½åˆ·æ–°ä¸–ç•Œç´€éŒ„",
        "è·æ¥­ç±ƒçƒè¯è³½å­£å¾Œè³½é–‹æ‰“",
        "ç¶²çƒå››å¤§æ»¿è²«è³½äº‹ç²¾å½©å°æ±º",
        "æ£’çƒä¸–ç•Œç¶“å…¸è³½åœ‹å®¶éšŠé›†è¨“",
        "é¦¬æ‹‰æ¾åœ‹éš›è³½äº‹å¸å¼•è¬äººåƒèˆ‡",
        "ç¾½æ¯›çƒå…¬é–‹è³½å† è»çˆ­å¥ªæˆ°",

        # å¥åº·ç›¸é—œ
        "æ–°å† ç–«è‹—æ¥ç¨®è¨ˆç•«å…¨é¢å•Ÿå‹•",
        "é†«é™¢å¼•é€²æœ€æ–°ç™Œç—‡æ²»ç™‚æŠ€è¡“",
        "å¥åº·é£²é£Ÿç¿’æ…£é é˜²æ…¢æ€§ç–¾ç—…",
        "å¿ƒç†å¥åº·è«®å•†æœå‹™éœ€æ±‚å¢åŠ ",
        "é‹å‹•å‚·å®³é˜²è­·çŸ¥è­˜å®£å°æ´»å‹•",
        "è€å¹´ç…§è­·æ”¿ç­–ç²å¾—ç¤¾æœƒé—œæ³¨",
        "é†«ç™‚å™¨æå‰µæ–°æŠ€è¡“ç²å¾—èªè­‰",
        "ç–«æƒ…é˜²æ§æªæ–½èª¿æ•´æœ€æ–°æ¶ˆæ¯",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - æ”¿æ²»+ç¶“æ¿Ÿ
        "æ”¿åºœæŠ•è³‡åŸºç¤å»ºè¨­ä¿ƒé€²ç¶“æ¿Ÿå¾©ç”¦",
        "å¤®è¡Œè²¨å¹£æ”¿ç­–å½±éŸ¿è‚¡å¸‚è¡¨ç¾",
        "åœ‹éš›è²¿æ˜“è«‡åˆ¤é—œä¿‚ç¶“æ¿Ÿç™¼å±•",
        "ç¨…åˆ¶æ”¹é©å°ä¼æ¥­ç‡Ÿé‹ç”¢ç”Ÿå½±éŸ¿",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - ç§‘æŠ€+ç¶“æ¿Ÿ
        "ç§‘æŠ€å…¬å¸è‚¡åƒ¹å› AIç™¼å±•å¤§æ¼²",
        "é›»å‹•è»Šç”¢æ¥­ç²å¾—æ”¿åºœæŠ•è³‡æ”¯æŒ",
        "æ•¸ä½è²¨å¹£æŠ€è¡“æ¨å‹•é‡‘èå‰µæ–°",
        "åŠå°é«”ç”¢æ¥­éˆä¾›æ‡‰çŸ­ç¼ºå•é¡Œ",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - é«”è‚²+ç¶“æ¿Ÿ
        "è·æ¥­é‹å‹•å“¡ç°½ç´„ä»£è¨€è²»å‰µæ–°é«˜",
        "é«”è‚²è³½äº‹è½‰æ’­æ¬Šåƒ¹æ ¼æŒçºŒä¸Šæ¼²",
        "é‹å‹•ç”¢æ¥­å¸¶å‹•ç›¸é—œç¶“æ¿Ÿç™¼å±•",
        "é›»ç«¶æ¯”è³½çé‡‘æ± çªç ´æ­·å²ç´€éŒ„",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - å¥åº·+ç§‘æŠ€
        "AIé†«ç™‚è¨ºæ–·ç³»çµ±æé«˜æº–ç¢ºç‡",
        "ç©¿æˆ´å¼å¥åº·ç›£æ¸¬è¨­å‚™æ™®åŠåŒ–",
        "é è·é†«ç™‚æœå‹™æŠ€è¡“ä¸æ–·é€²æ­¥",
        "åŸºå› ç·¨è¼¯æŠ€è¡“æ²»ç™‚ç½•è¦‹ç–¾ç—…",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - ä¸‰é‡æ¨™ç±¤
        "æ”¿åºœæ¨å‹•å¥åº·ç§‘æŠ€ç”¢æ¥­ç™¼å±•è¨ˆç•«",  # æ”¿æ²»+å¥åº·+ç§‘æŠ€
        "é«”è‚²æ˜æ˜ŸæŠ•è³‡ç§‘æŠ€æ–°å‰µå…¬å¸è‚¡æ¬Š",  # é«”è‚²+ç§‘æŠ€+ç¶“æ¿Ÿ
        "å¤®è¡Œç ”ç™¼æ•¸ä½è²¨å¹£å€å¡Šéˆå¹³å°",   # æ”¿æ²»+ç¶“æ¿Ÿ+ç§‘æŠ€
        "é†«ç™‚ä¿å¥æ”¿ç­–ç²å¾—é ç®—æ”¯æŒ",      # æ”¿æ²»+å¥åº·+ç¶“æ¿Ÿ
    ]

    labels = [
        # æ”¿æ²»ç›¸é—œ
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»", "å¥åº·"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»", "ç’°å¢ƒ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],

        # ç¶“æ¿Ÿç›¸é—œ
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ", "æ”¿æ²»"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ", "æˆ¿åœ°ç”¢"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],

        # ç§‘æŠ€ç›¸é—œ
        ["ç§‘æŠ€", "å¥åº·", "AI"],
        ["ç§‘æŠ€", "é€šè¨Š"],
        ["ç§‘æŠ€", "é‡å­"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€", "AI"],
        ["ç§‘æŠ€", "é›²ç«¯"],

        # é«”è‚²ç›¸é—œ
        ["é«”è‚²", "ç±ƒçƒ"],
        ["é«”è‚²", "è¶³çƒ"],
        ["é«”è‚²", "æ¸¸æ³³"],
        ["é«”è‚²", "ç±ƒçƒ"],
        ["é«”è‚²", "ç¶²çƒ"],
        ["é«”è‚²", "æ£’çƒ"],
        ["é«”è‚²", "è·‘æ­¥"],
        ["é«”è‚²"],

        # å¥åº·ç›¸é—œ
        ["å¥åº·", "æ”¿æ²»"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·"],
        ["å¥åº·"],
        ["å¥åº·", "é«”è‚²"],
        ["å¥åº·", "æ”¿æ²»"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·", "æ”¿æ²»"],

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],

        ["ç§‘æŠ€", "ç¶“æ¿Ÿ", "AI"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ", "æ”¿æ²»"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ"],

        ["é«”è‚²", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],

        ["å¥åº·", "ç§‘æŠ€", "AI"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·", "ç§‘æŠ€"],

        # ä¸‰é‡æ¨™ç±¤
        ["æ”¿æ²»", "å¥åº·", "ç§‘æŠ€"],
        ["é«”è‚²", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ", "ç§‘æŠ€"],
        ["æ”¿æ²»", "å¥åº·", "ç¶“æ¿Ÿ"],
    ]

    return texts, labels

# 3. è‡ªå®šç¾©æ•¸æ“šé›†é¡
class MultiLabelTextDataset(Dataset):
    def __init__(self, texts, labels, text_processor, vectorizer=None, mlb=None, is_training=True):
        self.texts = texts
        self.text_processor = text_processor
        self.is_training = is_training

        # é è™•ç†æ–‡æœ¬
        processed_texts = [self.text_processor.process_text(text) for text in texts]

        if is_training:
            # è¨“ç·´æ™‚å»ºç«‹æ–°çš„å‘é‡åŒ–å™¨å’Œæ¨™ç±¤ç·¨ç¢¼å™¨
            self.vectorizer = TfidfVectorizer(
                max_features=2000,
                stop_words=None,
                ngram_range=(1, 2),  # ä½¿ç”¨ 1-2 å…ƒçµ„åˆ
                min_df=1
            )
            self.mlb = MultiLabelBinarizer()

            # æ–‡æœ¬å‘é‡åŒ–
            tfidf_matrix = self.vectorizer.fit_transform(processed_texts)
            self.X = tfidf_matrix.toarray()
            # æ¨™ç±¤ç·¨ç¢¼
            self.y = self.mlb.fit_transform(labels)
        else:
            # æ¸¬è©¦æ™‚ä½¿ç”¨å·²æœ‰çš„å‘é‡åŒ–å™¨å’Œç·¨ç¢¼å™¨
            self.vectorizer = vectorizer
            self.mlb = mlb
            tfidf_matrix = self.vectorizer.transform(processed_texts)
            self.X = tfidf_matrix.toarray()
            self.y = self.mlb.transform(labels)

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.X[idx]), torch.FloatTensor(self.y[idx])

# 4. ç°¡åŒ–ç‰ˆ Transformer é¢¨æ ¼æ¨¡å‹
class SimpleTransformerClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_labels, num_heads=8, dropout_rate=0.3):
        super(SimpleTransformerClassifier, self).__init__()

        # è¼¸å…¥æŠ•å½±å±¤
        self.input_projection = nn.Linear(input_size, hidden_size)

        # å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=num_heads,
            dropout=dropout_rate,
            batch_first=True
        )

        # å‰é¥‹ç¶²è·¯
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size * 2, hidden_size)
        )

        # å±¤æ­£è¦åŒ–
        self.layer_norm1 = nn.LayerNorm(hidden_size)
        self.layer_norm2 = nn.LayerNorm(hidden_size)

        # åˆ†é¡é ­
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 2, num_labels),
            nn.Sigmoid()
        )

        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # è¼¸å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch_size, hidden_size]

        # æ·»åŠ åºåˆ—ç¶­åº¦ç”¨æ–¼æ³¨æ„åŠ›æ©Ÿåˆ¶
        x = x.unsqueeze(1)  # [batch_size, 1, hidden_size]

        # å¤šé ­æ³¨æ„åŠ› + æ®˜å·®é€£æ¥
        attn_output, _ = self.multihead_attn(x, x, x)
        x = self.layer_norm1(x + attn_output)

        # å‰é¥‹ç¶²è·¯ + æ®˜å·®é€£æ¥
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)

        # ç§»é™¤åºåˆ—ç¶­åº¦ä¸¦åˆ†é¡
        x = x.squeeze(1)  # [batch_size, hidden_size]
        x = self.classifier(x)

        return x

# 5. è¨“ç·´å‡½æ•¸
def train_model(model, train_loader, val_loader, device, num_epochs=30, learning_rate=0.001):
    """è¨“ç·´æ¨¡å‹"""

    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.8, patience=5
    )

    criterion = nn.BCELoss()

    train_losses = []
    val_losses = []

    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0.0

        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(device), targets.to(device)

            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, targets)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # é©—è­‰éšæ®µ
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for data, targets in val_loader:
                data, targets = data.to(device), targets.to(device)
                outputs = model(data)
                loss = criterion(outputs, targets)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        # å­¸ç¿’ç‡èª¿åº¦
        scheduler.step(avg_val_loss)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss

        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}]')
            print(f'  Train Loss: {avg_train_loss:.4f}')
            print(f'  Val Loss: {avg_val_loss:.4f}')
            print()

    return train_losses, val_losses

# 6. è©•ä¼°å‡½æ•¸
def evaluate_model(model, test_loader, mlb, device, threshold=0.5):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    all_predictions = []
    all_targets = []
    all_probabilities = []

    with torch.no_grad():
        for data, targets in test_loader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)

            probabilities = outputs.cpu().numpy()
            predictions = (probabilities > threshold).astype(int)

            all_predictions.extend(predictions)
            all_targets.extend(targets.cpu().numpy())
            all_probabilities.extend(probabilities)

    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)

    # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
    hamming = hamming_loss(all_targets, all_predictions)
    jaccard = jaccard_score(all_targets, all_predictions, average='samples', zero_division=0)
    f1_micro = f1_score(all_targets, all_predictions, average='micro', zero_division=0)
    f1_macro = f1_score(all_targets, all_predictions, average='macro', zero_division=0)

    print("ğŸ” æ¨¡å‹è©•ä¼°çµæœï¼š")
    print(f"  Hamming Loss: {hamming:.4f}")
    print(f"  Jaccard Score: {jaccard:.4f}")
    print(f"  F1 Score (Micro): {f1_micro:.4f}")
    print(f"  F1 Score (Macro): {f1_macro:.4f}")
    print()

    # è©³ç´°åˆ†é¡å ±å‘Š
    print("ğŸ“Š å„æ¨™ç±¤è©³ç´°å ±å‘Šï¼š")
    print(classification_report(
        all_targets, all_predictions,
        target_names=mlb.classes_,
        zero_division=0
    ))

    return all_predictions, all_targets, all_probabilities

# 7. é æ¸¬æ–°æ–‡æœ¬
def predict_text(model, text, text_processor, vectorizer, mlb, device, threshold=0.3):
    """é æ¸¬å–®å€‹æ–‡æœ¬çš„æ¨™ç±¤"""
    model.eval()

    # é è™•ç†æ–‡æœ¬
    processed_text = text_processor.process_text(text)

    # å‘é‡åŒ–
    text_vector = vectorizer.transform([processed_text]).toarray()
    text_tensor = torch.FloatTensor(text_vector).to(device)

    with torch.no_grad():
        outputs = model(text_tensor)
        probabilities = outputs.cpu().numpy()[0]

    # è‡ªé©æ‡‰é–¾å€¼
    predictions = (probabilities > threshold).astype(int)
    if predictions.sum() == 0:  # å¦‚æœæ²’æœ‰é æ¸¬åˆ°ä»»ä½•æ¨™ç±¤
        # é¸æ“‡æ©Ÿç‡æœ€é«˜çš„2å€‹æ¨™ç±¤
        top_indices = np.argsort(probabilities)[-2:]
        predictions[top_indices] = 1

    predicted_labels = []
    for i, pred in enumerate(predictions):
        if pred == 1:
            predicted_labels.append(mlb.classes_[i])

    # é¡¯ç¤ºçµæœ
    print(f"\nğŸ“ æ–‡æœ¬: '{text}'")
    print(f"ğŸ”§ è™•ç†å¾Œ: '{processed_text}'")
    print(f"ğŸ¯ é–¾å€¼: {threshold}")
    print("ğŸ“ˆ é æ¸¬çµæœï¼ˆæŒ‰æ©Ÿç‡æ’åºï¼‰ï¼š")

    # æŒ‰æ©Ÿç‡æ’åºé¡¯ç¤º
    prob_indices = np.argsort(probabilities)[::-1]
    for i in prob_indices:
        status = "âœ“" if predictions[i] == 1 else "âœ—"
        print(f"  {status} {mlb.classes_[i]}: {probabilities[i]:.3f}")

    print(f"ğŸ·ï¸  æœ€çµ‚é æ¸¬æ¨™ç±¤: {predicted_labels}")
    return predicted_labels

# 8. ç¹ªè£½è¨“ç·´æ›²ç·š
def plot_training_curves(train_losses, val_losses):
    """ç¹ªè£½è¨“ç·´å’Œé©—è­‰æå¤±æ›²ç·š"""
    plt.figure(figsize=(12, 5))

    plt.plot(train_losses, 'b-', label='Training Loss', linewidth=2)
    plt.plot(val_losses, 'r-', label='Validation Loss', linewidth=2)
    plt.title('Transformer-style Training Curves', fontsize=14)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('transformer_training_curves.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜ç‚º 'transformer_training_curves.png'")

# 9. ä¸»å‡½æ•¸
def main():
    # è¨­å®šè¨­å‚™
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {device}")

    # åˆå§‹åŒ–æ–‡æœ¬è™•ç†å™¨
    if JIEBA_AVAILABLE:
        print("âœ… jieba ä¸­æ–‡åˆ†è©å·¥å…·å·²å®‰è£")
    else:
        print("âŒ jieba æœªå®‰è£ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆæ–‡æœ¬è™•ç†")
        print("ğŸ’¡ å¦‚éœ€æ›´å¥½æ•ˆæœï¼Œè«‹å®‰è£: pip install jieba")

    text_processor = ChineseTextProcessor()

    # æº–å‚™æ•¸æ“š
    print("\nğŸ“š æº–å‚™è¨“ç·´æ•¸æ“š...")
    texts, labels = create_comprehensive_data()
    print(f"âœ… å…± {len(texts)} å€‹è¨“ç·´æ¨£æœ¬")

    # åˆ†å‰²æ•¸æ“š
    train_texts, temp_texts, train_labels, temp_labels = train_test_split(
        texts, labels, test_size=0.4, random_state=42
    )

    val_texts, test_texts, val_labels, test_labels = train_test_split(
        temp_texts, temp_labels, test_size=0.5, random_state=42
    )

    print(f"ğŸ“Š æ•¸æ“šåˆ†å‰²: è¨“ç·´ {len(train_texts)}, é©—è­‰ {len(val_texts)}, æ¸¬è©¦ {len(test_texts)}")

    # å‰µå»ºæ•¸æ“šé›†
    train_dataset = MultiLabelTextDataset(
        train_texts, train_labels, text_processor, is_training=True
    )
    val_dataset = MultiLabelTextDataset(
        val_texts, val_labels, text_processor,
        train_dataset.vectorizer, train_dataset.mlb, is_training=False
    )
    test_dataset = MultiLabelTextDataset(
        test_texts, test_labels, text_processor,
        train_dataset.vectorizer, train_dataset.mlb, is_training=False
    )

    # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
    batch_size = 16
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # ç²å–æ¨¡å‹åƒæ•¸
    input_size = train_dataset.X.shape[1]
    num_labels = len(train_dataset.mlb.classes_)

    print(f"ğŸ·ï¸  è¼¸å…¥ç‰¹å¾µç¶­åº¦: {input_size}")
    print(f"ğŸ·ï¸  æ¨™ç±¤æ•¸é‡: {num_labels}")
    print(f"ğŸ·ï¸  æ¨™ç±¤é¡åˆ¥: {list(train_dataset.mlb.classes_)}")

    # å‰µå»ºæ¨¡å‹
    print(f"\nğŸ—ï¸  å‰µå»º Transformer é¢¨æ ¼æ¨¡å‹...")
    model = SimpleTransformerClassifier(
        input_size=input_size,
        hidden_size=256,
        num_labels=num_labels,
        num_heads=8,
        dropout_rate=0.3
    ).to(device)
    print("âœ… æ¨¡å‹å‰µå»ºæˆåŠŸ")

    # è¨“ç·´æ¨¡å‹
    print(f"\nğŸ¯ é–‹å§‹è¨“ç·´...")
    train_losses, val_losses = train_model(
        model, train_loader, val_loader, device,
        num_epochs=30, learning_rate=0.001
    )

    # ç¹ªè£½è¨“ç·´æ›²ç·š
    plot_training_curves(train_losses, val_losses)

    # è©•ä¼°æ¨¡å‹
    print("\nğŸ” è©•ä¼°æ¨¡å‹...")
    predictions, targets, probabilities = evaluate_model(
        model, test_loader, train_dataset.mlb, device
    )

    # æ¸¬è©¦æ–°æ–‡æœ¬é æ¸¬
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦æ–°æ–‡æœ¬é æ¸¬ï¼š")

    test_texts_new = [
        "æ”¿åºœå®£å¸ƒç§‘æŠ€ç”¢æ¥­æ¸›ç¨…æ”¿ç­–åˆºæ¿€ç¶“æ¿Ÿç™¼å±•",
        "NBAçƒæ˜ŸæŠ•è³‡å€å¡Šéˆæ–°å‰µå…¬å¸ç²å¾—è±åšå›å ±",
        "å¤®è¡Œæ¨å‡ºæ•¸ä½è²¨å¹£æŠ€è¡“ç ”ç™¼è¨ˆç•«",
        "äººå·¥æ™ºæ…§é†«ç™‚è¨ºæ–·ç³»çµ±æé«˜æ²»ç™‚æ•ˆæœ",
        "é›»ç«¶é¸æ‰‹åœ¨äºé‹æœƒå¥ªé‡‘å‰µé€ æ­·å²",
        "ç¶ èƒ½ç§‘æŠ€è‚¡ç¥¨å› æ”¿ç­–åˆ©å¤šå¤§æ¼²",
        "è·æ¥­è¶³çƒå“¡ç°½ç´„é‹å‹•å“ç‰Œä»£è¨€åˆç´„",
        "é è·é†«ç™‚æœå‹™æŠ€è¡“ç²å¾—æŠ•è³‡é—œæ³¨",
        "é‡å­è¨ˆç®—çªç ´å¸¶å‹•ç›¸é—œç”¢æ¥­ç™¼å±•",
        "é«”è‚²ç”¢æ¥­æ•¸ä½è½‰å‹ç²æ”¿åºœæ”¯æŒ"
    ]

    for text in test_texts_new:
        predict_text(
            model, text, text_processor,
            train_dataset.vectorizer, train_dataset.mlb,
            device, threshold=0.3
        )

    print(f"\nğŸ‰ Transformer é¢¨æ ¼å¤šæ¨™ç±¤åˆ†é¡è¨“ç·´å®Œæˆï¼")

if __name__ == "__main__":
    main()