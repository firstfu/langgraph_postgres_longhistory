# =============================================================================
# åŸºæ–¼ LLM å¾®èª¿çš„å¤šæ¨™ç±¤æ–‡æœ¬åˆ†é¡æ¨¡å‹
# åŠŸèƒ½ï¼šä½¿ç”¨é è¨“ç·´çš„ä¸­æ–‡èªè¨€æ¨¡å‹é€²è¡Œå¤šæ¨™ç±¤åˆ†é¡
# æŠ€è¡“ï¼šBERT/RoBERTa + å¾®èª¿ + å¤šæ¨™ç±¤åˆ†é¡é ­
# å„ªå‹¢ï¼šæ›´å¥½çš„èªç¾©ç†è§£èƒ½åŠ›ï¼Œæ”¯æ´ä¸­æ–‡èªè¨€ç‰¹æ€§
# =============================================================================

import matplotlib

matplotlib.use('Agg')  # ä½¿ç”¨éäº’å‹•å¼å¾Œç«¯é¿å…é¡¯ç¤ºå•é¡Œ
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from scipy import sparse
from sklearn.metrics import (accuracy_score, classification_report, f1_score,
                             hamming_loss, jaccard_score)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
# è™•ç†ä¸åŒç‰ˆæœ¬çš„ transformers å¥—ä»¶åŒ¯å…¥
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
from transformers import AutoConfig, AutoModel, AutoTokenizer
from transformers.optimization import get_linear_schedule_with_warmup

warnings.filterwarnings('ignore')

# è¨­å®šéš¨æ©Ÿç¨®å­ç¢ºä¿çµæœå¯é‡ç¾
torch.manual_seed(42)
np.random.seed(42)

# 1. æº–å‚™è¨“ç·´æ•¸æ“š
def create_comprehensive_data():
    """å‰µå»ºæ›´è±å¯Œçš„å¤šæ¨™ç±¤è¨“ç·´æ•¸æ“š"""
    texts = [
        # æ”¿æ²»ç›¸é—œ
        "ç¸½çµ±å®£å¸ƒæ–°çš„ç¶“æ¿Ÿåˆºæ¿€æ”¿ç­–ä»¥ä¿ƒé€²åœ‹å®¶ç™¼å±•",
        "åœ‹æœƒè­°å“¡è¨è«–ä¿®æ”¹ç¨…æ”¶æ³•æ¡ˆçš„ç›¸é—œæ¢æ¬¾",
        "å¤–äº¤éƒ¨é•·èˆ‡å„åœ‹ä»£è¡¨èˆ‰è¡Œé‡è¦æœƒè«‡",
        "æ”¿åºœæ¨å‡ºå…¨æ°‘å¥åº·ä¿éšªæ”¹é©æ–¹æ¡ˆ",
        "é¸èˆ‰å§”å“¡æœƒå…¬å¸ƒæœ€æ–°çš„æ°‘æ„èª¿æŸ¥çµæœ",
        "ç«‹æ³•é™¢é€šéæ–°çš„ç’°å¢ƒä¿è­·æ³•æ¡ˆ",
        "å¸‚é•·å®£å¸ƒéƒ½å¸‚æ›´æ–°è¨ˆç•«æŠ•è³‡é‡‘é¡",
        "ç¸½ç†å‡ºè¨ªé„°åœ‹è¨è«–è²¿æ˜“åˆä½œå”è­°",

        # ç¶“æ¿Ÿç›¸é—œ
        "è‚¡å¸‚ä»Šæ—¥æ”¶ç›¤å‰µä¸‹æ­·å²æ–°é«˜ç´€éŒ„",
        "å¤®è¡Œå®£å¸ƒèª¿é™åŸºæº–åˆ©ç‡åˆºæ¿€ç¶“æ¿Ÿ",
        "åœ‹éš›æ²¹åƒ¹ä¸Šæ¼²å½±éŸ¿é€šè†¨é æœŸ",
        "æˆ¿åœ°ç”¢å¸‚å ´äº¤æ˜“é‡å¤§å¹…å¢åŠ ",
        "ä¼æ¥­è²¡å ±é¡¯ç¤ºç²åˆ©å¤§å¹…æˆé•·",
        "å°±æ¥­å¸‚å ´å‡ºç¾æ˜é¡¯å¾©ç”¦è·¡è±¡",
        "æ¶ˆè²»è€…ç‰©åƒ¹æŒ‡æ•¸æŒçºŒä¸Šå‡",
        "åŒ¯ç‡æ³¢å‹•å°å‡ºå£ç”¢æ¥­é€ æˆè¡æ“Š",

        # ç§‘æŠ€ç›¸é—œ
        "äººå·¥æ™ºæ…§æŠ€è¡“åœ¨é†«ç™‚è¨ºæ–·é ˜åŸŸå–å¾—çªç ´",
        "5Gç¶²è·¯å»ºè¨­å¸¶å‹•ç›¸é—œç”¢æ¥­ç™¼å±•",
        "é‡å­è¨ˆç®—ç ”ç©¶ç²å¾—é‡å¤§é€²å±•",
        "å€å¡ŠéˆæŠ€è¡“æ‡‰ç”¨æ–¼é‡‘èæœå‹™å‰µæ–°",
        "è‡ªå‹•é§•é§›æ±½è»Šå®Œæˆè·¯æ¸¬é©—è­‰",
        "è™›æ“¬å¯¦å¢ƒè¨­å‚™éŠ·é‡å‰µæ–°é«˜",
        "æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å„ªåŒ–ç”Ÿç”¢æ•ˆç‡",
        "é›²ç«¯é‹ç®—æœå‹™å¸‚å ´å¿«é€Ÿæ“´å¼µ",

        # é«”è‚²ç›¸é—œ
        "NBAç¸½å† è»è³½é€²å…¥æ±ºæˆ°æ™‚åˆ»",
        "ä¸–ç•Œç›ƒè¶³çƒè³½å°çµ„è³½æ¿€çƒˆç«¶çˆ­",
        "å¥§é‹æ¸¸æ³³æ¯”è³½åˆ·æ–°ä¸–ç•Œç´€éŒ„",
        "è·æ¥­ç±ƒçƒè¯è³½å­£å¾Œè³½é–‹æ‰“",
        "ç¶²çƒå››å¤§æ»¿è²«è³½äº‹ç²¾å½©å°æ±º",
        "æ£’çƒä¸–ç•Œç¶“å…¸è³½åœ‹å®¶éšŠé›†è¨“",
        "é¦¬æ‹‰æ¾åœ‹éš›è³½äº‹å¸å¼•è¬äººåƒèˆ‡",
        "ç¾½æ¯›çƒå…¬é–‹è³½å† è»çˆ­å¥ªæˆ°",

        # å¥åº·ç›¸é—œ
        "æ–°å† ç–«è‹—æ¥ç¨®è¨ˆç•«å…¨é¢å•Ÿå‹•",
        "é†«é™¢å¼•é€²æœ€æ–°ç™Œç—‡æ²»ç™‚æŠ€è¡“",
        "å¥åº·é£²é£Ÿç¿’æ…£é é˜²æ…¢æ€§ç–¾ç—…",
        "å¿ƒç†å¥åº·è«®å•†æœå‹™éœ€æ±‚å¢åŠ ",
        "é‹å‹•å‚·å®³é˜²è­·çŸ¥è­˜å®£å°æ´»å‹•",
        "è€å¹´ç…§è­·æ”¿ç­–ç²å¾—ç¤¾æœƒé—œæ³¨",
        "é†«ç™‚å™¨æå‰µæ–°æŠ€è¡“ç²å¾—èªè­‰",
        "ç–«æƒ…é˜²æ§æªæ–½èª¿æ•´æœ€æ–°æ¶ˆæ¯",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - æ”¿æ²»+ç¶“æ¿Ÿ
        "æ”¿åºœæŠ•è³‡åŸºç¤å»ºè¨­ä¿ƒé€²ç¶“æ¿Ÿå¾©ç”¦",
        "å¤®è¡Œè²¨å¹£æ”¿ç­–å½±éŸ¿è‚¡å¸‚è¡¨ç¾",
        "åœ‹éš›è²¿æ˜“è«‡åˆ¤é—œä¿‚ç¶“æ¿Ÿç™¼å±•",
        "ç¨…åˆ¶æ”¹é©å°ä¼æ¥­ç‡Ÿé‹ç”¢ç”Ÿå½±éŸ¿",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - ç§‘æŠ€+ç¶“æ¿Ÿ
        "ç§‘æŠ€å…¬å¸è‚¡åƒ¹å› AIç™¼å±•å¤§æ¼²",
        "é›»å‹•è»Šç”¢æ¥­ç²å¾—æ”¿åºœæŠ•è³‡æ”¯æŒ",
        "æ•¸ä½è²¨å¹£æŠ€è¡“æ¨å‹•é‡‘èå‰µæ–°",
        "åŠå°é«”ç”¢æ¥­éˆä¾›æ‡‰çŸ­ç¼ºå•é¡Œ",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - é«”è‚²+ç¶“æ¿Ÿ
        "è·æ¥­é‹å‹•å“¡ç°½ç´„ä»£è¨€è²»å‰µæ–°é«˜",
        "é«”è‚²è³½äº‹è½‰æ’­æ¬Šåƒ¹æ ¼æŒçºŒä¸Šæ¼²",
        "é‹å‹•ç”¢æ¥­å¸¶å‹•ç›¸é—œç¶“æ¿Ÿç™¼å±•",
        "é›»ç«¶æ¯”è³½çé‡‘æ± çªç ´æ­·å²ç´€éŒ„",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - å¥åº·+ç§‘æŠ€
        "AIé†«ç™‚è¨ºæ–·ç³»çµ±æé«˜æº–ç¢ºç‡",
        "ç©¿æˆ´å¼å¥åº·ç›£æ¸¬è¨­å‚™æ™®åŠåŒ–",
        "é è·é†«ç™‚æœå‹™æŠ€è¡“ä¸æ–·é€²æ­¥",
        "åŸºå› ç·¨è¼¯æŠ€è¡“æ²»ç™‚ç½•è¦‹ç–¾ç—…",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬ - ä¸‰é‡æ¨™ç±¤
        "æ”¿åºœæ¨å‹•å¥åº·ç§‘æŠ€ç”¢æ¥­ç™¼å±•è¨ˆç•«",  # æ”¿æ²»+å¥åº·+ç§‘æŠ€
        "é«”è‚²æ˜æ˜ŸæŠ•è³‡ç§‘æŠ€æ–°å‰µå…¬å¸è‚¡æ¬Š",  # é«”è‚²+ç§‘æŠ€+ç¶“æ¿Ÿ
        "å¤®è¡Œç ”ç™¼æ•¸ä½è²¨å¹£å€å¡Šéˆå¹³å°",   # æ”¿æ²»+ç¶“æ¿Ÿ+ç§‘æŠ€
        "é†«ç™‚ä¿å¥æ”¿ç­–ç²å¾—é ç®—æ”¯æŒ",      # æ”¿æ²»+å¥åº·+ç¶“æ¿Ÿ
    ]

    labels = [
        # æ”¿æ²»ç›¸é—œ
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»", "å¥åº·"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»", "ç’°å¢ƒ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],

        # ç¶“æ¿Ÿç›¸é—œ
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ", "æ”¿æ²»"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ", "æˆ¿åœ°ç”¢"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],

        # ç§‘æŠ€ç›¸é—œ
        ["ç§‘æŠ€", "å¥åº·", "AI"],
        ["ç§‘æŠ€", "é€šè¨Š"],
        ["ç§‘æŠ€", "é‡å­"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€", "AI"],
        ["ç§‘æŠ€", "é›²ç«¯"],

        # é«”è‚²ç›¸é—œ
        ["é«”è‚²", "ç±ƒçƒ"],
        ["é«”è‚²", "è¶³çƒ"],
        ["é«”è‚²", "æ¸¸æ³³"],
        ["é«”è‚²", "ç±ƒçƒ"],
        ["é«”è‚²", "ç¶²çƒ"],
        ["é«”è‚²", "æ£’çƒ"],
        ["é«”è‚²", "è·‘æ­¥"],
        ["é«”è‚²"],

        # å¥åº·ç›¸é—œ
        ["å¥åº·", "æ”¿æ²»"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·"],
        ["å¥åº·"],
        ["å¥åº·", "é«”è‚²"],
        ["å¥åº·", "æ”¿æ²»"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·", "æ”¿æ²»"],

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],

        ["ç§‘æŠ€", "ç¶“æ¿Ÿ", "AI"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ", "æ”¿æ²»"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ"],

        ["é«”è‚²", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],

        ["å¥åº·", "ç§‘æŠ€", "AI"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·", "ç§‘æŠ€"],

        # ä¸‰é‡æ¨™ç±¤
        ["æ”¿æ²»", "å¥åº·", "ç§‘æŠ€"],
        ["é«”è‚²", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ", "ç§‘æŠ€"],
        ["æ”¿æ²»", "å¥åº·", "ç¶“æ¿Ÿ"],
    ]

    return texts, labels

# 2. è‡ªå®šç¾©æ•¸æ“šé›†é¡
class MultiLabelTextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, mlb=None, max_length=128, is_training=True):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.is_training = is_training

        if is_training:
            self.mlb = MultiLabelBinarizer()
            labels_encoded = self.mlb.fit_transform(labels)
            # ç¢ºä¿è½‰æ›ç‚ºå¯†é›†æ•¸çµ„æ ¼å¼
            if sparse.issparse(labels_encoded):
                self.labels = labels_encoded.toarray()  # type: ignore
            else:
                self.labels = labels_encoded
        else:
            self.mlb = mlb
            labels_encoded = self.mlb.transform(labels)
            # ç¢ºä¿è½‰æ›ç‚ºå¯†é›†æ•¸çµ„æ ¼å¼
            if sparse.issparse(labels_encoded):
                self.labels = labels_encoded.toarray()  # type: ignore
            else:
                self.labels = labels_encoded

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])

        # ä½¿ç”¨ tokenizer ç·¨ç¢¼æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.FloatTensor(self.labels[idx])  # type: ignore
        }

# 3. å¤šæ¨™ç±¤åˆ†é¡æ¨¡å‹
class LLMMultiLabelClassifier(nn.Module):
    def __init__(self, model_name, num_labels, dropout_rate=0.3):
        super(LLMMultiLabelClassifier, self).__init__()

        # è¼‰å…¥é è¨“ç·´æ¨¡å‹é…ç½®
        self.config = AutoConfig.from_pretrained(model_name)
        self.backbone = AutoModel.from_pretrained(model_name)

        # åˆ†é¡é ­
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.config.hidden_size, num_labels)

        # å‡çµéƒ¨åˆ†é è¨“ç·´å±¤ï¼ˆå¯é¸ï¼‰
        # self._freeze_layers()

    def _freeze_layers(self):
        """å‡çµå‰å¹¾å±¤ä»¥ç©©å®šè¨“ç·´"""
        for param in self.backbone.embeddings.parameters():
            param.requires_grad = False

        for layer in self.backbone.encoder.layer[:8]:  # å‡çµå‰8å±¤
            for param in layer.parameters():
                param.requires_grad = False

    def forward(self, input_ids, attention_mask):
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # ä½¿ç”¨ [CLS] token çš„è¡¨ç¤º
        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        return torch.sigmoid(logits)  # å¤šæ¨™ç±¤åˆ†é¡ä½¿ç”¨ sigmoid

# 4. è¨“ç·´å‡½æ•¸
def train_model(model, train_loader, val_loader, device, num_epochs=10, learning_rate=2e-5):
    """è¨“ç·´ LLM å¤šæ¨™ç±¤åˆ†é¡æ¨¡å‹"""

    # å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨
    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)

    total_steps = len(train_loader) * num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),
        num_training_steps=total_steps
    )

    criterion = nn.BCELoss()

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0.0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()

            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()

            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # é©—è­‰éšæ®µ
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        print(f'Epoch [{epoch+1}/{num_epochs}]')
        print(f'  Train Loss: {avg_train_loss:.4f}')
        print(f'  Val Loss: {avg_val_loss:.4f}')
        print()

    return train_losses, val_losses

# 5. è©•ä¼°å‡½æ•¸
def evaluate_model(model, test_loader, mlb, device, threshold=0.5):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    all_predictions = []
    all_targets = []
    all_probabilities = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels']

            outputs = model(input_ids, attention_mask)
            probabilities = outputs.cpu().numpy()
            predictions = (probabilities > threshold).astype(int)

            all_predictions.extend(predictions)
            all_targets.extend(labels.numpy())
            all_probabilities.extend(probabilities)

    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)

    # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
    hamming = hamming_loss(all_targets, all_predictions)
    jaccard = jaccard_score(all_targets, all_predictions, average='samples', zero_division=0)
    f1_micro = f1_score(all_targets, all_predictions, average='micro', zero_division=0)
    f1_macro = f1_score(all_targets, all_predictions, average='macro', zero_division=0)

    print("ğŸ” æ¨¡å‹è©•ä¼°çµæœï¼š")
    print(f"  Hamming Loss: {hamming:.4f}")
    print(f"  Jaccard Score: {jaccard:.4f}")
    print(f"  F1 Score (Micro): {f1_micro:.4f}")
    print(f"  F1 Score (Macro): {f1_macro:.4f}")
    print()

    # è©³ç´°åˆ†é¡å ±å‘Š
    print("ğŸ“Š å„æ¨™ç±¤è©³ç´°å ±å‘Šï¼š")
    print(classification_report(
        all_targets, all_predictions,
        target_names=mlb.classes_,
        zero_division=0
    ))

    return all_predictions, all_targets, all_probabilities

# 6. é æ¸¬æ–°æ–‡æœ¬
def predict_text(model, tokenizer, text, mlb, device, threshold=0.3):
    """é æ¸¬å–®å€‹æ–‡æœ¬çš„æ¨™ç±¤"""
    model.eval()

    # ç·¨ç¢¼æ–‡æœ¬
    encoding = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=128,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask)
        probabilities = outputs.cpu().numpy()[0]

    # è‡ªé©æ‡‰é–¾å€¼
    predictions = (probabilities > threshold).astype(int)
    if predictions.sum() == 0:  # å¦‚æœæ²’æœ‰é æ¸¬åˆ°ä»»ä½•æ¨™ç±¤
        # é¸æ“‡æ©Ÿç‡æœ€é«˜çš„æ¨™ç±¤
        top_indices = np.argsort(probabilities)[-2:]  # é¸æ“‡å‰2å€‹æœ€é«˜æ©Ÿç‡çš„æ¨™ç±¤
        predictions[top_indices] = 1

    predicted_labels = []
    for i, pred in enumerate(predictions):
        if pred == 1:
            predicted_labels.append(mlb.classes_[i])

    # é¡¯ç¤ºçµæœ
    print(f"\nğŸ“ æ–‡æœ¬: '{text}'")
    print(f"ğŸ¯ é–¾å€¼: {threshold}")
    print("ğŸ“ˆ é æ¸¬çµæœï¼ˆæŒ‰æ©Ÿç‡æ’åºï¼‰ï¼š")

    # æŒ‰æ©Ÿç‡æ’åºé¡¯ç¤º
    prob_indices = np.argsort(probabilities)[::-1]
    for i in prob_indices:
        status = "âœ“" if predictions[i] == 1 else "âœ—"
        print(f"  {status} {mlb.classes_[i]}: {probabilities[i]:.3f}")

    print(f"ğŸ·ï¸  æœ€çµ‚é æ¸¬æ¨™ç±¤: {predicted_labels}")
    return predicted_labels

# 7. ç¹ªè£½è¨“ç·´æ›²ç·š
def plot_training_curves(train_losses, val_losses):
    """ç¹ªè£½è¨“ç·´å’Œé©—è­‰æå¤±æ›²ç·š"""
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 1, 1)
    plt.plot(train_losses, 'b-', label='Training Loss', linewidth=2)
    plt.plot(val_losses, 'r-', label='Validation Loss', linewidth=2)
    plt.title('Training and Validation Loss', fontsize=14)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('llm_training_curves.png', dpi=300, bbox_inches='tight')
    plt.close()
    print("ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜ç‚º 'llm_training_curves.png'")

# 8. ä¸»å‡½æ•¸
def main():
    # è¨­å®šè¨­å‚™
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {device}")

    # é¸æ“‡é è¨“ç·´æ¨¡å‹ï¼ˆä¸­æ–‡ BERTï¼‰
    model_name = "bert-base-chinese"  # æˆ–ä½¿ç”¨ "hfl/chinese-roberta-wwm-ext"
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")

    try:
        # è¼‰å…¥ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("âœ… Tokenizer è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è¼‰å…¥ tokenizer å¤±æ•—: {e}")
        print("ğŸ’¡ è«‹å…ˆå®‰è£ transformers: pip install transformers")
        return

    # æº–å‚™æ•¸æ“š
    print("\nğŸ“š æº–å‚™è¨“ç·´æ•¸æ“š...")
    texts, labels = create_comprehensive_data()
    print(f"âœ… å…± {len(texts)} å€‹è¨“ç·´æ¨£æœ¬")

    # åˆ†å‰²æ•¸æ“š
    train_texts, temp_texts, train_labels, temp_labels = train_test_split(
        texts, labels, test_size=0.4, random_state=42, stratify=None
    )

    val_texts, test_texts, val_labels, test_labels = train_test_split(
        temp_texts, temp_labels, test_size=0.5, random_state=42
    )

    print(f"ğŸ“Š æ•¸æ“šåˆ†å‰²: è¨“ç·´ {len(train_texts)}, é©—è­‰ {len(val_texts)}, æ¸¬è©¦ {len(test_texts)}")

    # å‰µå»ºæ•¸æ“šé›†
    train_dataset = MultiLabelTextDataset(train_texts, train_labels, tokenizer, is_training=True)
    val_dataset = MultiLabelTextDataset(val_texts, val_labels, tokenizer, train_dataset.mlb, is_training=False)
    test_dataset = MultiLabelTextDataset(test_texts, test_labels, tokenizer, train_dataset.mlb, is_training=False)

    # å‰µå»ºæ•¸æ“šè¼‰å…¥å™¨
    batch_size = 8
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # ç²å–æ¨™ç±¤ä¿¡æ¯
    num_labels = len(train_dataset.mlb.classes_)
    print(f"ğŸ·ï¸  æ¨™ç±¤æ•¸é‡: {num_labels}")
    print(f"ğŸ·ï¸  æ¨™ç±¤é¡åˆ¥: {list(train_dataset.mlb.classes_)}")

    # å‰µå»ºæ¨¡å‹
    print(f"\nğŸ—ï¸  å‰µå»ºæ¨¡å‹...")
    model = LLMMultiLabelClassifier(model_name, num_labels).to(device)
    print("âœ… æ¨¡å‹å‰µå»ºæˆåŠŸ")

    # è¨“ç·´æ¨¡å‹
    print(f"\nğŸ¯ é–‹å§‹è¨“ç·´...")
    train_losses, val_losses = train_model(
        model, train_loader, val_loader, device,
        num_epochs=5, learning_rate=2e-5
    )

    # ç¹ªè£½è¨“ç·´æ›²ç·š
    plot_training_curves(train_losses, val_losses)

    # è©•ä¼°æ¨¡å‹
    print("\nğŸ” è©•ä¼°æ¨¡å‹...")
    predictions, targets, probabilities = evaluate_model(
        model, test_loader, train_dataset.mlb, device
    )

    # æ¸¬è©¦æ–°æ–‡æœ¬é æ¸¬
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦æ–°æ–‡æœ¬é æ¸¬ï¼š")

    test_texts_new = [
        "æ”¿åºœå®£å¸ƒç§‘æŠ€ç”¢æ¥­æ¸›ç¨…æ”¿ç­–åˆºæ¿€ç¶“æ¿Ÿç™¼å±•",
        "NBAçƒæ˜ŸæŠ•è³‡å€å¡Šéˆæ–°å‰µå…¬å¸ç²å¾—è±åšå›å ±",
        "å¤®è¡Œæ¨å‡ºæ•¸ä½è²¨å¹£æŠ€è¡“ç ”ç™¼è¨ˆç•«",
        "äººå·¥æ™ºæ…§é†«ç™‚è¨ºæ–·ç³»çµ±æé«˜æ²»ç™‚æ•ˆæœ",
        "é›»ç«¶é¸æ‰‹åœ¨äºé‹æœƒå¥ªé‡‘å‰µé€ æ­·å²",
        "ç¶ èƒ½ç§‘æŠ€è‚¡ç¥¨å› æ”¿ç­–åˆ©å¤šå¤§æ¼²",
        "è·æ¥­è¶³çƒå“¡ç°½ç´„é‹å‹•å“ç‰Œä»£è¨€åˆç´„",
        "é è·é†«ç™‚æœå‹™æŠ€è¡“ç²å¾—æŠ•è³‡é—œæ³¨",
        "é‡å­è¨ˆç®—çªç ´å¸¶å‹•ç›¸é—œç”¢æ¥­ç™¼å±•",
        "é«”è‚²ç”¢æ¥­æ•¸ä½è½‰å‹ç²æ”¿åºœæ”¯æŒ"
    ]

    for text in test_texts_new:
        predict_text(model, tokenizer, text, train_dataset.mlb, device, threshold=0.3)

    print(f"\nğŸ‰ LLM å¤šæ¨™ç±¤åˆ†é¡è¨“ç·´å®Œæˆï¼")

    # ä¿å­˜æ¨¡å‹ï¼ˆå¯é¸ï¼‰
    # torch.save(model.state_dict(), 'llm_multilabel_model.pth')
    # print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜")

if __name__ == "__main__":
    main()