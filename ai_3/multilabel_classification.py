# =============================================================================
# å¤šæ¨™ç±¤æ–‡æœ¬åˆ†é¡æ¨¡å‹ - ä½¿ç”¨PyTorchå¯¦ç¾
# åŠŸèƒ½ï¼šå°ä¸­æ–‡æ–‡æœ¬é€²è¡Œå¤šæ¨™ç±¤åˆ†é¡ï¼Œæ”¯æ´ä¸€å€‹æ–‡æœ¬å°æ‡‰å¤šå€‹æ¨™ç±¤
# æŠ€è¡“ï¼šTF-IDFç‰¹å¾µæå– + ç¥ç¶“ç¶²è·¯åˆ†é¡å™¨
# =============================================================================

import matplotlib

matplotlib.use('Agg')  # ä½¿ç”¨éäº’å‹•å¼å¾Œç«¯é¿å…é¡¯ç¤ºå•é¡Œ
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from scipy import sparse  # æ·»åŠ  scipy ç¨€ç–çŸ©é™£æ”¯æ´
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (classification_report, f1_score, hamming_loss,
                             jaccard_score)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from torch.utils.data import DataLoader, Dataset

# è¨­å®šéš¨æ©Ÿç¨®å­ç¢ºä¿çµæœå¯é‡ç¾
torch.manual_seed(42)
np.random.seed(42)

# 1. æº–å‚™ç¯„ä¾‹æ•¸æ“š
def create_sample_data():
    """å‰µå»ºç¯„ä¾‹æ–‡æœ¬æ•¸æ“šå’Œå¤šæ¨™ç±¤"""
    texts = [
        # æ”¿æ²»ç›¸é—œ
        "ç¸½çµ±ä»Šå¤©ç™¼è¡¨ç¶“æ¿Ÿæ”¿ç­–æ¼”èªª",
        "åœ‹æœƒé€šéæ–°æ³•æ¡ˆ",
        "å¸‚é•·å®£å¸ƒéƒ½å¸‚æ›´æ–°è¨ˆç•«",
        "å¤–äº¤éƒ¨é•·æœƒè¦‹å„åœ‹å¤§ä½¿",
        "æ”¿åºœæ¨å‡ºç¤¾æœƒç¦åˆ©æ”¿ç­–",
        "é¸èˆ‰æŠ•ç¥¨ç‡å‰µæ–°é«˜",
        "ç«‹æ³•é™¢å¯©æŸ¥é ç®—æ¡ˆ",
        "ç¸½ç†è¨ªå•å‹é‚¦åœ‹å®¶",

        # é«”è‚²ç›¸é—œ
        "NBAç¸½å† è»è³½ä»Šæ™šé–‹æ‰“",
        "è¶³çƒä¸–ç•Œç›ƒç²¾å½©é€²çƒé›†éŒ¦",
        "å¥§é‹æœƒæ¸¸æ³³æ¯”è³½ç ´ç´€éŒ„",
        "ç±ƒçƒæ˜æ˜Ÿè½‰éšŠå¼•ç™¼é—œæ³¨",
        "ç¶²çƒå…¬é–‹è³½æ±ºè³½å°æˆ°",
        "æ£’çƒå­£å¾Œè³½æ¿€çƒˆç«¶çˆ­",
        "é¦¬æ‹‰æ¾æ¯”è³½å‰µä½³ç¸¾",
        "ç¾½æ¯›çƒå† è»è³½ç²¾å½©å°æ±º",
        "é«˜çˆ¾å¤«çƒå…¬é–‹è³½é–‹æ‰“",
        "æ’çƒè¯è³½ç¸½æ±ºè³½",

        # ç§‘æŠ€ç›¸é—œ
        "äººå·¥æ™ºæ…§æŠ€è¡“çªç ´å‰µæ–°é«˜",
        "5Gç¶²è·¯å»ºè¨­åŠ é€Ÿæ¨é€²",
        "é‡å­è¨ˆç®—ç ”ç©¶æ–°é€²å±•",
        "é›²ç«¯æœå‹™å¸‚å ´æ“´å¼µ",
        "æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å„ªåŒ–",
        "å€å¡ŠéˆæŠ€è¡“æ–°æ‡‰ç”¨",
        "è™›æ“¬å¯¦å¢ƒç”¢å“ç™¼å¸ƒ",
        "è‡ªå‹•é§•é§›æŠ€è¡“æ¸¬è©¦",
        "æ™ºæ…§å‹æ‰‹æ©Ÿæ–°åŠŸèƒ½",
        "ç‰©è¯ç¶²è¨­å‚™æ™®åŠåŒ–",

        # ç¶“æ¿Ÿç›¸é—œ
        "è‚¡å¸‚ä»Šæ—¥å¤§æ¼²ç§‘æŠ€è‚¡é ˜æ¼²",
        "å¤®è¡Œèª¿æ•´åˆ©ç‡æ”¿ç­–",
        "åœ‹éš›è²¿æ˜“å”è­°è«‡åˆ¤",
        "æˆ¿åœ°ç”¢å¸‚å ´è¶¨å‹¢åˆ†æ",
        "é€šè†¨ç‡æŒçºŒä¸Šå‡",
        "å°±æ¥­ç‡å‰µæ­·å²æ–°é«˜",
        "åŒ¯ç‡æ³¢å‹•å½±éŸ¿å‡ºå£",
        "æ¶ˆè²»è€…ä¿¡å¿ƒæŒ‡æ•¸ä¸‹æ»‘",
        "ä¼æ¥­è²¡å ±è¡¨ç¾äº®çœ¼",
        "ç¶“æ¿Ÿæˆé•·ç‡è¶…å‡ºé æœŸ",

        # å¥åº·ç›¸é—œ
        "æ–°å† ç–«æƒ…é˜²æ§æªæ–½æ›´æ–°",
        "é†«é™¢æ¨å‡ºå¥åº·æª¢æŸ¥æ–¹æ¡ˆ",
        "ç–«è‹—æ¥ç¨®ç‡é”åˆ°ç›®æ¨™",
        "æ–°è—¥ç²å¾—æ”¿åºœæ ¸å‡†",
        "é†«ç™‚è¨­å‚™æŠ€è¡“å‡ç´š",
        "å¥åº·é£²é£Ÿè§€å¿µæ¨å»£",
        "é‹å‹•å‚·å®³é é˜²å®£å°",
        "å¿ƒç†å¥åº·è«®å•†æœå‹™",

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬
        "æ”¿åºœæŠ•è³‡ç§‘æŠ€ç”¢æ¥­ä¿ƒé€²ç¶“æ¿Ÿç™¼å±•",  # æ”¿æ²»+ç§‘æŠ€+ç¶“æ¿Ÿ
        "NBAçƒæ˜ŸæŠ•è³‡å€å¡Šéˆæ–°å‰µå…¬å¸",  # é«”è‚²+ç±ƒçƒ+ç§‘æŠ€+ç¶“æ¿Ÿ
        "é†«ç™‚AIè¨ºæ–·æŠ€è¡“ç²æ”¿åºœè£œåŠ©",  # å¥åº·+ç§‘æŠ€+æ”¿æ²»
        "é›»ç«¶æ¯”è³½çé‡‘å‰µæ–°é«˜ç´€éŒ„",  # é«”è‚²+ç§‘æŠ€+ç¶“æ¿Ÿ
        "å¤®è¡Œç ”ç™¼æ•¸ä½è²¨å¹£æŠ€è¡“",  # ç¶“æ¿Ÿ+ç§‘æŠ€+æ”¿æ²»
        "é‹å‹•å“¡ä»£è¨€å¥åº·é£Ÿå“å»£å‘Š",  # é«”è‚²+å¥åº·+ç¶“æ¿Ÿ
        "æ™ºæ…§é†«ç™‚è¨­å‚™éŠ·é‡å¤§å¢",  # ç§‘æŠ€+å¥åº·+ç¶“æ¿Ÿ
        "æ”¿åºœæ¨å‹•é«”è‚²ç”¢æ¥­ç™¼å±•",  # æ”¿æ²»+é«”è‚²+ç¶“æ¿Ÿ
        "æˆ¿åœ°ç”¢ç§‘æŠ€æ‡‰ç”¨è¶¨å‹¢",  # æˆ¿åœ°ç”¢+ç§‘æŠ€+ç¶“æ¿Ÿ
        "å¥åº·ç§‘æŠ€è‚¡ç¥¨è¡¨ç¾å¼·å‹",  # å¥åº·+ç§‘æŠ€+ç¶“æ¿Ÿ
        "é«”è‚²æ˜æ˜ŸæŠ•è³‡æˆ¿åœ°ç”¢",  # é«”è‚²+æˆ¿åœ°ç”¢+ç¶“æ¿Ÿ
        "æ”¿åºœæ¨å»£é‹å‹•å¥åº·æ”¿ç­–",  # æ”¿æ²»+é«”è‚²+å¥åº·
        "AIæŠ€è¡“æ”¹å–„é†«ç™‚è¨ºæ–·",  # ç§‘æŠ€+AI+å¥åº·
        "é›»å‹•è»Šç”¢æ¥­æ”¿ç­–æ”¯æŒ",  # ç§‘æŠ€+ç¶“æ¿Ÿ+æ”¿æ²»
        "é‹å‹•å™¨æç§‘æŠ€å‰µæ–°",  # é«”è‚²+ç§‘æŠ€
    ]

    # æ¯å€‹æ–‡æœ¬å°æ‡‰çš„å¤šå€‹æ¨™ç±¤
    labels = [
        # æ”¿æ²»ç›¸é—œ
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»", "æˆ¿åœ°ç”¢"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»", "å¥åº·"],
        ["æ”¿æ²»"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»"],

        # é«”è‚²ç›¸é—œ
        ["é«”è‚²", "ç±ƒçƒ"],
        ["é«”è‚²", "è¶³çƒ"],
        ["é«”è‚²", "æ¸¸æ³³"],
        ["é«”è‚²", "ç±ƒçƒ"],
        ["é«”è‚²", "ç¶²çƒ"],
        ["é«”è‚²", "æ£’çƒ"],
        ["é«”è‚²", "è·‘æ­¥"],
        ["é«”è‚²"],
        ["é«”è‚²"],
        ["é«”è‚²"],

        # ç§‘æŠ€ç›¸é—œ
        ["ç§‘æŠ€", "AI"],
        ["ç§‘æŠ€", "é€šè¨Š"],
        ["ç§‘æŠ€", "é‡å­"],
        ["ç§‘æŠ€", "é›²ç«¯"],
        ["ç§‘æŠ€", "AI"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€"],
        ["ç§‘æŠ€"],

        # ç¶“æ¿Ÿç›¸é—œ
        ["ç¶“æ¿Ÿ", "ç§‘æŠ€"],
        ["ç¶“æ¿Ÿ", "æ”¿æ²»"],
        ["æ”¿æ²»", "ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ", "æˆ¿åœ°ç”¢"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ"],

        # å¥åº·ç›¸é—œ
        ["å¥åº·", "æ”¿æ²»"],
        ["å¥åº·"],
        ["å¥åº·", "æ”¿æ²»"],
        ["å¥åº·"],
        ["å¥åº·", "ç§‘æŠ€"],
        ["å¥åº·"],
        ["å¥åº·", "é«”è‚²"],
        ["å¥åº·"],

        # å¤šæ¨™ç±¤è¤‡åˆæ–‡æœ¬
        ["æ”¿æ²»", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "ç±ƒçƒ", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["å¥åº·", "ç§‘æŠ€", "æ”¿æ²»"],
        ["é«”è‚²", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["ç¶“æ¿Ÿ", "ç§‘æŠ€", "æ”¿æ²»"],
        ["é«”è‚²", "å¥åº·", "ç¶“æ¿Ÿ"],
        ["ç§‘æŠ€", "å¥åº·", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "é«”è‚²", "ç¶“æ¿Ÿ"],
        ["æˆ¿åœ°ç”¢", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["å¥åº·", "ç§‘æŠ€", "ç¶“æ¿Ÿ"],
        ["é«”è‚²", "æˆ¿åœ°ç”¢", "ç¶“æ¿Ÿ"],
        ["æ”¿æ²»", "é«”è‚²", "å¥åº·"],
        ["ç§‘æŠ€", "AI", "å¥åº·"],
        ["ç§‘æŠ€", "ç¶“æ¿Ÿ", "æ”¿æ²»"],
        ["é«”è‚²", "ç§‘æŠ€"],
    ]

    return texts, labels

# 2. è‡ªå®šç¾©æ•¸æ“šé›†é¡
class MultiLabelTextDataset(Dataset):
    def __init__(self, texts, labels, vectorizer=None, mlb=None, is_training=True):
        self.texts = texts
        self.labels = labels
        self.is_training = is_training

        if is_training:
            # è¨“ç·´æ™‚å»ºç«‹æ–°çš„å‘é‡åŒ–å™¨å’Œæ¨™ç±¤ç·¨ç¢¼å™¨
            self.vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
            self.mlb = MultiLabelBinarizer()

            # æ–‡æœ¬å‘é‡åŒ– - ç¢ºä¿è½‰æ›ç‚º numpy å¯†é›†çŸ©é™£
            tfidf_sparse = self.vectorizer.fit_transform(texts)
            self.X = tfidf_sparse.toarray()  # ç›´æ¥è½‰æ›ç‚ºå¯†é›†çŸ©é™£
            # æ¨™ç±¤ç·¨ç¢¼
            self.y = self.mlb.fit_transform(labels)
        else:
            # æ¸¬è©¦æ™‚ä½¿ç”¨å·²æœ‰çš„å‘é‡åŒ–å™¨å’Œç·¨ç¢¼å™¨
            self.vectorizer = vectorizer
            self.mlb = mlb
            tfidf_sparse = self.vectorizer.transform(texts)
            self.X = tfidf_sparse.toarray()  # ç›´æ¥è½‰æ›ç‚ºå¯†é›†çŸ©é™£
            self.y = self.mlb.transform(labels)

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.X[idx]), torch.FloatTensor(self.y[idx])

# 3. å®šç¾©å¤šæ¨™ç±¤åˆ†é¡æ¨¡å‹
class MultiLabelClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_labels, dropout_rate=0.2):
        super(MultiLabelClassifier, self).__init__()

        # ç°¡åŒ–ç¶²è·¯æ¶æ§‹ï¼Œé˜²æ­¢éåº¦æ“¬åˆ
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.BatchNorm1d(hidden_size),  # æ·»åŠ æ‰¹æ¬¡æ­£è¦åŒ–
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.BatchNorm1d(hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size // 2, num_labels),
            nn.Sigmoid()  # é—œéµï¼šä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•¸
        )

    def forward(self, x):
        return self.network(x)

# 4. è¨“ç·´å‡½æ•¸
def train_model(model, train_loader, criterion, optimizer, num_epochs, device):
    model.train()
    train_losses = []

    for epoch in range(num_epochs):
        running_loss = 0.0
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(device), targets.to(device)

            # æ¸…é›¶æ¢¯åº¦
            optimizer.zero_grad()

            # å‰å‘å‚³æ’­
            outputs = model(data)

            # è¨ˆç®—æå¤±
            loss = criterion(outputs, targets)

            # åå‘å‚³æ’­
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        epoch_loss = running_loss / len(train_loader)
        train_losses.append(epoch_loss)

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')

    return train_losses

# 5. è©•ä¼°å‡½æ•¸
def evaluate_model(model, test_loader, mlb, device, threshold=0.5):
    model.eval()
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for data, targets in test_loader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)

            # æ ¹æ“šé–¾å€¼é€²è¡Œé æ¸¬
            predictions = (outputs > threshold).float()

            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)

    # è¨ˆç®—è©•ä¼°æŒ‡æ¨™
    hamming = hamming_loss(all_targets, all_predictions)
    jaccard = jaccard_score(all_targets, all_predictions, average='samples', zero_division=0)
    f1_micro = f1_score(all_targets, all_predictions, average='micro', zero_division=0)
    f1_macro = f1_score(all_targets, all_predictions, average='macro', zero_division=0)

    print(f"\nè©•ä¼°çµæœï¼š")
    print(f"Hamming Loss: {hamming:.4f}")
    print(f"Jaccard Score: {jaccard:.4f}")
    print(f"F1 Score (Micro): {f1_micro:.4f}")
    print(f"F1 Score (Macro): {f1_macro:.4f}")

    # é¡¯ç¤ºæ¯å€‹æ¨™ç±¤çš„è©³ç´°å ±å‘Š
    print(f"\nå„æ¨™ç±¤è©³ç´°å ±å‘Šï¼š")
    print(classification_report(all_targets, all_predictions,
                              target_names=mlb.classes_, zero_division=0))

    return all_predictions, all_targets

# 6. é æ¸¬æ–°æ–‡æœ¬çš„å‡½æ•¸
def predict_new_text(model, text, vectorizer, mlb, device, threshold=0.3):
    """é æ¸¬æ–°æ–‡æœ¬çš„æ¨™ç±¤"""
    model.eval()

    # å‘é‡åŒ–æ–°æ–‡æœ¬
    text_sparse = vectorizer.transform([text])
    text_vector = text_sparse.toarray()  # è½‰æ›ç‚ºnumpyå¯†é›†çŸ©é™£
    text_tensor = torch.FloatTensor(text_vector).to(device)

    with torch.no_grad():
        output = model(text_tensor)
        probabilities = output.cpu().numpy()[0]

        # ä½¿ç”¨å¤šç¨®é–¾å€¼é€²è¡Œé æ¸¬
        thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]
        best_threshold = threshold

        # å¦‚æœåŸå§‹é–¾å€¼æ²’æœ‰é æ¸¬åˆ°ä»»ä½•æ¨™ç±¤ï¼Œå˜—è©¦è¼ƒä½çš„é–¾å€¼
        original_predictions = (probabilities > threshold).astype(int)
        if original_predictions.sum() == 0:
            for t in [0.2, 0.15, 0.1, 0.05]:
                test_predictions = (probabilities > t).astype(int)
                if test_predictions.sum() > 0:
                    best_threshold = t
                    break

        predictions = (probabilities > best_threshold).astype(int)

    # ç²å–é æ¸¬çš„æ¨™ç±¤
    predicted_labels = mlb.inverse_transform(predictions.reshape(1, -1))[0]

    # é¡¯ç¤ºçµæœ
    print(f"\næ–‡æœ¬: '{text}'")
    print(f"ä½¿ç”¨é–¾å€¼: {best_threshold}")
    print("é æ¸¬æ¨™ç±¤åŠæ©Ÿç‡ï¼š")

    # æ’åºé¡¯ç¤ºï¼ˆæŒ‰æ©Ÿç‡å¾é«˜åˆ°ä½ï¼‰
    prob_with_labels = list(zip(mlb.classes_, probabilities, predictions))
    prob_with_labels.sort(key=lambda x: x[1], reverse=True)

    for label, prob, pred in prob_with_labels:
        status = "âœ“" if pred == 1 else "âœ—"
        print(f"  {status} {label}: {prob:.3f}")
    print(f"æœ€çµ‚é æ¸¬æ¨™ç±¤: {list(predicted_labels)}")

    return predicted_labels

# 7. ç¹ªè£½è¨“ç·´æå¤±æ›²ç·š
def plot_training_loss(train_losses):
    """ç¹ªè£½ä¸¦ä¿å­˜è¨“ç·´æå¤±æ›²ç·š"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, 'b-', linewidth=2)
    plt.title('Training Loss Curve', fontsize=14)  # ä½¿ç”¨è‹±æ–‡é¿å…å­—é«”å•é¡Œ
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.grid(True, alpha=0.3)

    # ä¿å­˜åœ–ç‰‡è€Œä¸æ˜¯é¡¯ç¤º
    plt.savefig('training_loss.png', dpi=300, bbox_inches='tight')
    plt.close()  # é—œé–‰åœ–å½¢é‡‹æ”¾è¨˜æ†¶é«”
    print("ğŸ“Š è¨“ç·´æå¤±æ›²ç·šå·²ä¿å­˜ç‚º 'training_loss.png'")

# 8. ä¸»å‡½æ•¸
def main():
    # è¨­å®šè¨­å‚™
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    import sys
    sys.stdout.flush()  # å¼·åˆ¶åˆ·æ–°è¼¸å‡º

    # æº–å‚™æ•¸æ“š
    print("æº–å‚™æ•¸æ“š...")
    texts, labels = create_sample_data()

    # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“š
    train_texts, test_texts, train_labels, test_labels = train_test_split(
        texts, labels, test_size=0.3, random_state=42
    )

    # å‰µå»ºæ•¸æ“šé›†
    train_dataset = MultiLabelTextDataset(train_texts, train_labels, is_training=True)
    test_dataset = MultiLabelTextDataset(test_texts, test_labels,
                                       train_dataset.vectorizer,
                                       train_dataset.mlb,
                                       is_training=False)

    # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨ - èª¿æ•´æ‰¹æ¬¡å¤§å°
    batch_size = min(16, len(train_dataset) // 3)  # å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # ç²å–è¼¸å…¥ç¶­åº¦å’Œæ¨™ç±¤æ•¸é‡
    input_size = train_dataset.X.shape[1]
    num_labels = len(train_dataset.mlb.classes_)

    print(f"è¼¸å…¥ç‰¹å¾µç¶­åº¦: {input_size}")
    print(f"æ¨™ç±¤æ•¸é‡: {num_labels}")
    print(f"æ¨™ç±¤é¡åˆ¥: {list(train_dataset.mlb.classes_)}")

    # å‰µå»ºæ¨¡å‹ - é™ä½æ¨¡å‹è¤‡é›œåº¦
    model = MultiLabelClassifier(input_size, 128, num_labels).to(device)  # é™ä½hidden_sizeå¾256åˆ°128

    # å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨ - èª¿æ•´å­¸ç¿’ç‡
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)  # æé«˜å­¸ç¿’ç‡å’Œweight_decay

    # è¨“ç·´æ¨¡å‹ - æ¸›å°‘è¨“ç·´é€±æœŸ
    print("\né–‹å§‹è¨“ç·´...")
    train_losses = train_model(model, train_loader, criterion, optimizer, 50, device)  # é™ä½epochså¾100åˆ°50

    # ç¹ªè£½è¨“ç·´æå¤±
    plot_training_loss(train_losses)

    # è©•ä¼°æ¨¡å‹
    print("\nè©•ä¼°æ¨¡å‹...")
    predictions, targets = evaluate_model(model, test_loader, train_dataset.mlb, device)

    # æ¸¬è©¦æ–°æ–‡æœ¬é æ¸¬
    print("\n" + "="*50)
    print("æ¸¬è©¦æ–°æ–‡æœ¬é æ¸¬ï¼š")
    test_texts_new = [
        # æ˜ç¢ºçš„å¤šæ¨™ç±¤æ–‡æœ¬
        "æ”¿åºœå®£å¸ƒç§‘æŠ€ç”¢æ¥­æ¸›ç¨…æ”¿ç­–åˆºæ¿€ç¶“æ¿Ÿç™¼å±•",  # æ”¿æ²»+ç¶“æ¿Ÿ+ç§‘æŠ€
        "NBAçƒæ˜ŸæŠ•è³‡å€å¡Šéˆæ–°å‰µå…¬å¸",  # é«”è‚²+ç±ƒçƒ+ç§‘æŠ€+ç¶“æ¿Ÿ
        "å¤®è¡Œæ•¸ä½è²¨å¹£æŠ€è¡“ç ”ç™¼é€²å±•é †åˆ©",  # ç¶“æ¿Ÿ+æ”¿æ²»+ç§‘æŠ€
        "é›»ç«¶é¸æ‰‹åœ¨äºé‹æœƒå¥ªé‡‘å¼•ç™¼æŠ•è³‡ç†±æ½®",  # é«”è‚²+ç§‘æŠ€+ç¶“æ¿Ÿ

        # å–®ä¸€é ˜åŸŸæ–‡æœ¬
        "é†«é™¢æ¨å‡ºæ–°å† è‚ºç‚å¿«ç¯©æœå‹™",  # å¥åº·
        "æˆ¿åœ°ç”¢å¸‚å ´å‡ºç¾å›æº«è·¡è±¡",  # ç¶“æ¿Ÿ+æˆ¿åœ°ç”¢
        "å­¸æ ¡åœèª²æ”¹ç‚ºç·šä¸Šæ•™å­¸æ¨¡å¼",  # æ•™è‚²+ç§‘æŠ€
        "ç’°ä¿åœ˜é«”æŠ—è­°å·¥å» æ±¡æŸ“å•é¡Œ",  # ç’°å¢ƒ+æ”¿æ²»

        # é«”è‚²ç›¸é—œå¤šæ¨™ç±¤
        "è¶³çƒæ˜æ˜Ÿç°½ç´„ä»£è¨€é‹å‹•å“ç‰Œåˆç´„",  # é«”è‚²+è¶³çƒ+ç¶“æ¿Ÿ
        "é¦¬æ‹‰æ¾è³½äº‹å¸¶å‹•è§€å…‰ç”¢æ¥­ç™¼å±•",  # é«”è‚²+è·‘æ­¥+ç¶“æ¿Ÿ

        # å…¶ä»–å¤šæ¨£åŒ–å…§å®¹
        "éŸ³æ¨‚ç¯€é–€ç¥¨éŠ·å”®å‰µæ­·å²æ–°é«˜",  # å¨›æ¨‚+ç¶“æ¿Ÿ
        "æ°£å€™è®Šé·å½±éŸ¿è¾²æ¥­ç”Ÿç”¢æ”¿ç­–"   # ç’°å¢ƒ+æ”¿æ²»+ç¶“æ¿Ÿ
    ]

    # ä½¿ç”¨è¼ƒä½çš„é–¾å€¼ä¾†æé«˜å¤šæ¨™ç±¤é æ¸¬çš„å¯èƒ½æ€§
    for text in test_texts_new:
        predict_new_text(model, text, train_dataset.vectorizer,
                        train_dataset.mlb, device, threshold=0.3)  # é™ä½é–¾å€¼å¾0.5åˆ°0.3

    print("\nè¨“ç·´å®Œæˆï¼")

if __name__ == "__main__":
    main()